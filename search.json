[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Web Archive Manager (WAM)",
    "section": "",
    "text": "The Web Archive Manager (WAM) is part of the toolkit to aid researchers in working with web archives. Our aim is to enable usage of archives as a research data source.\nWAM allows you to organise, replay, and annotate your archive files (.wacz or .warc). You can also use WAM to scrape data into structured formats.\nWAM is developed by QUT Digital Observatory, as part of the Australian Internet Observatory (AIO). AIO received co-investment (doi.org/10.3565/hjrp-b141) from the Australian Research Data Commons (ARDC) through the HASS and Indigenous Research Data Commons. The ARDC is enabled by the National Collaborative Research Infrastructure Strategy (NCRIS).\nPlease share your feedback by emailing us at digitalobservatory@qut.edu.au."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Web Archive Manager (WAM)",
    "section": "Installation",
    "text": "Installation\nWAM is currently available in Windows only. Click on the link below to download the installer.\n\nWindowsMacOSLinux\n\n\nWAM (Web Archive Manager)-0.0.1a\n\n\nCurrently unavailable\n\n\nCurrently unavailable"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Web Archive Manager (WAM)",
    "section": "Getting started",
    "text": "Getting started\n\nNaming conventions for metadata titles\nMetadata titles are case sensitive, meaning metadataTitle and metadatatitle will be taken as two different words.\nNo spaces or special characters are allowed. The only special character you can use is @.\nStandard metadata such as file, timestamp, size, and tags are reserved, so if you try to add a metadata title using these words, they will be ignored.\nExamples of acceptable metadata titles: authorName, authorname, @authorName."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Why web archives?",
    "section": "",
    "text": "A lot of research use data sourced from the World Wide Web, and it is easy to see why. An unimaginable amount of data exist on the Web, much of is public and freely available. Web data are anything that we can find on the Web. They can be content from social media platforms, news sites, message boards, and review forums.\nOn the other hand, Web data’s ever-changing nature pose unique challenges to researchers, whose standards of research require data to be reproducible and replicable. Content on the Web can be easily modified or removed, meaning that data collected at one time might not be same if collected another time. Research that use Web data need to employ data collection methods that allow for this property of Web data."
  },
  {
    "objectID": "about.html#traditional-methods",
    "href": "about.html#traditional-methods",
    "title": "Why web archives?",
    "section": "Traditional methods",
    "text": "Traditional methods\nOne way that many researchers have used is taking screenshots of the Web page(s), using either their computer’s screenshot tool or the numerous browser extensions (e.g. GoFullPage). This is a quick and intuitive way of capturing snapshots of Web content at specific points in time. However, screenshots suffer from several major limitations. First, screenshots are usually static files (PDF or images), which do not capture the interactive elements of web pages (e.g. links, navigations, pop-ups). Second, screenshots can also be challenging to organise. For example, researchers analysing pages from different review forums may have to manually group the screenshots by forums. And lastly, web scraping of structured data cannot be done from the screenshot."
  },
  {
    "objectID": "about.html#web-archiving-as-a-data-collection-method",
    "href": "about.html#web-archiving-as-a-data-collection-method",
    "title": "Why web archives?",
    "section": "Web archiving as a data collection method",
    "text": "Web archiving as a data collection method\nWeb archiving has been used since the 90s, largely for preserving Web data in an archival format. Only recently has its potential as a research data collection technique been recognised. Web archives, for the most part, retain all features of a web page. This means that it captures all interactive elements on a page, and users can interact with the archived page as they would a live page. Web scraping can also be done on an archive.\n\nThe WARC and WACZ file formats\nArchived content are stored in WARC file format, which tends to be bundled with other files into a WACZ (Web Archive Collection Zipped) file. The WARC and WACZ file formats contain not only the web data, but also other metadata (archived time and content type) that can aid data provenance. WARC and more recently WACZ are recognised as the standard for web archiving, so these can be saved and replayed using free tools such as the Wayback Machine or ReplayWeb."
  },
  {
    "objectID": "about.html#tools-for-web-archiving",
    "href": "about.html#tools-for-web-archiving",
    "title": "Why web archives?",
    "section": "Tools for web archiving",
    "text": "Tools for web archiving\n\nWayback Machine\nThe Wayback Machine is the most well-known public archive of the Web. The Waymachine Machine began archiving the Internet in 1996, preserving all public Web content at regular intervals. This allows you to see how a web page changes over time.\nThe Wayback Machine also offers useful tools to archive web pages of your choice, provided that they are public and not hidden behind login screens or paywalls. The easiest way to do so is via the WM browser extension. All archived pages are stored publicly and can be accessed by anyone, although you can download a local copy of the archive file (WACZ) to your laptop.\n\n\nWebrecorder\nFor private pages or sensitive data that cannot be public, the Webcorder offers a suite of tools for you to make your own archives. These free Chrome browser extentions enable you to archive pages and replay them.\n\n\npywb\nResearchers with programmatic skills can use pywb for maximum customisation and automation of the archiving process."
  }
]